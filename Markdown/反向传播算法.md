# 反向传播算法简介（BP，Backpropagation algorithm）

原文：[http://neuralnetworksanddeeplearning.com/chap2.html][0]

BP 算法所关注的是神经网络中损失函数 C (cost function) 与每一个权重 $w$ 和偏置 $b$ 的偏导。BP 不仅仅是一个快速的算法，其同时为我们提供了一个视角，让我们观察权值和偏置是如何影响网络输出的。

译者注：本文中所描述的网络以层为单位，如果把层当做图的节点，数据流向作为图的有向边，那么本文所描述的网络所抽象出的图一定是有向无环的。___本文并没有翻译原文所有内容___。

## 热身：利用矩阵实现网络计算

先介绍一个网络权重的数学标记法：$w_{jk}^l$，这个数学标记表示神经网络中第 $l$ 层网络的第  $j$ 个元素和第 $l-1$ 层第 $k$ 个元素之间的权重。同样，$b_j^l$ 表示第 $l$ 层第 $j$ 个元素的偏置值，$a_j^l$ 表示 $l$ 层第 $j$ 个元素的激活函数输出值。利用这种数学表示，$a_j^l$ 可以表示为：

$$
a_j^l = \sigma(\sum_k w_{jk}^l a_k^{l-1} + b_j^l) \tag{1}
$$

使用矩阵形式表示上述表达式：

$$
a^l = \sigma(w^l a^{l-1} + b^l) \tag{2}
$$

定义 $z^l = w^l a^{l-1} + b^l$ 为激活函数的输入值则可以将上面表达式$(2)$ 表示为：
$$
a^l = \sigma(z^l) \tag{3}
$$

## 损失函数的两个特点

BP 算法用于计算网络中所有权重 $w$ 和偏置 $b$ 关于损失函数 $C$ 的偏导数 $\frac{\partial C}{\partial w}$和$\frac{\partial C}{\partial b}$。为了使 BP 算法正常运行，损失函数需要满足两个条件。在给出这两个条件前，我们先介绍一种常用的均方差损失函数，如式$(4)$所示：
$$
C = \frac 1{2n} \sum_x \Vert y(x) - a^L(x) \Vert ^2 \tag{4}
$$
表达式 $(4)$ 中变量解释如下， $n$ 是训练网络的样本个数；$y(x)$ 是训练样本 $x$ 的期望值（也就是样本的标签）；$L$ 表示网络的层数；$a^L  = a^L(x)$ 是网络在输入为 $x$ 时输出层的输出。

现在描述我们对损失函数的要求。首先，损失函数可以写成所有训练样本损失值均值的形式：$C = \frac {1}{n} \sum_x C_x$ 。

我们做上面的要求是因为训练的过程中我们常常使用批训练的方式，而不是每次只训练一个样本。批训练之后我们求当前批次样本损失值的平均数来更新权重和偏置，所以损失函数要满足叠加定理。

其次，损失函数可以使用网络输出层的输出作为参数：$\C = C(a^L)$，$a^L$ 是网络输出层的输出，如果不满足这个要求我们将不能定量分析网络的性能。以均方差损失函数为例，当样本为$x$时，网络的损失值为：
$$
C = \frac 1 2 \Vert y-a^L \Vert ^2 = \frac 1 2 \sum_j(y_j-a_j^L)^2 \tag{5}
$$
上式中所有元素的值都是已知的，$y$ 是标签、$a_j^L$ 是网络输出层的输出。

## Hadamard 积，$s \odot t $

Hadamardd 积（哈达玛积）表示矩阵按对应元素做乘法：$(s \odot t)_j = s_j t_j$，例如：
$$
\begin{vmatrix}
	1 \\
	2 
\end{vmatrix} \odot \begin{vmatrix}
	3 \\
	4 
\end{vmatrix} = \begin{vmatrix}
	1*3 \\
	2*4 
\end{vmatrix} = \begin{vmatrix}
	3 \\
	8 
\end{vmatrix} \tag{6}
$$

## BP 算法所依赖的四个方程

BP 算法用于计算网络中权值与偏置关于网络损失值的偏导，也就是计算：$\frac{\partial C}{\partial w_{jk}^l}$和$\frac{\partial C}{\partial b_j^l}$ 。在给出这四个方程前我们先引入一个中间变量 $\delta_j^l$，这个变量表示网络第 $l$ 层第 $j$ 个元素的输入值（$z_j^l$）对整个网络损失的影响。BP 算法可以帮我们计算出 $\delta_j^l$ ，然后我们就可以通过 $\delta_j^l$ 得到$\frac{\partial C}{\partial w_{jk}^l}$和$\frac{\partial C}{\partial b_j^l}$ 。

为了便于理解，这里我们假设网络中的某个神经元（第 $l$ 层第$j$ 个）旁边住着一个小恶魔，它可以帮我们调整神经元的输入（$z_j^l$）。小恶魔不出手时这个神经元的输入为：$ z_j^l$，小恶魔出手后，当前神经元的输入为：$ z_j^l + \Delta z_j^l$，其中$\Delta z_j^l$ 是小恶魔做的调整，此时这个神经元的输出为 $\sigma (z_j^l + \Delta z_j^l)$。小恶魔对当前神经元的调整最终将影响整个网络的输出，小恶魔出手后对整个网络损失值的影响为：$\frac {\partial C} {\partial z_j^l} \Delta z_j^l$。

如果这是一个善良的小恶魔，那么它可以帮我们调整输入以减少网络的损失值。如果$\frac {\partial C} {\partial z_j^l}$ 的值不为0，那么调整$ z_j^l = z_j^l - \eta \frac {\partial C} {\partial z_j^l} \Delta z_j^l$，将减小整个网络的损失值。如果$\frac {\partial C} {\partial z_j^l}$的值为0，那么小恶魔的调整对网络损失的贡献就非常小，因为导数为0，再大的调整对网络都没有影响（这里暂不考虑舍入误差）。

我们定义 $\delta_j^l$ 如下：
$$
\delta \equiv \frac {\partial C} {\partial z_j^l} \tag{7}
$$
式$(6)$的矩阵表示方法为：$\delta ^l$。BP 算法可以帮助我们计算网络中每一层的 $\delta ^l$。

### 第一个方程：输出层损失值计算

网络输出层 $\delta ^L$的计算方式为：
$$
\delta_j^L = \frac {\partial C}{\partial a_j^L} \sigma{'}(z_j^l)\tag{BP1}
$$
BP1 等号右侧偏导部分表示网络输出层最后的输出对网络损失的影响强度，$\sigma$ 的导数表示这个神经元输入对整个网络损失的影响强度（下有证明）。

需要注意的是对于最后一层而言，BP1 的结果是很容易计算的。$\frac {\partial C}{\partial a_j^L}$依赖于损失函数的形式。举个例子，如果我们使用式$(5)$中的均方差作为最终的损失函数，那么$\frac {\partial C}{\partial a_j^L} = (a_j^L - y_j)$。

以矩阵的形式表示$(BP1)$：
$$
\delta^L = \nabla _aC \odot \sigma^{'}(z^L) \tag{BP1a}
$$
如果损失函数还是均方差的话，那么 $\nabla _aC = (a^L-y)$，从而可得：
$$
\delta^L = (a^L-y) \odot \sigma^{'}(z^L) \tag{8}
$$

#### 证明

$$
\begin{align}
\delta^L_j &= \frac{\partial C}{\partial z^L_j} \\
           &= \frac{\partial C}{\partial a^L_j} \frac{\partial a^L_j}{\partial z^L_j}\\
           &=\frac{\partial C}{\partial a^L_j} \sigma'(z^L_j)
\end{align}
$$

### 第二个方程：使用下一层网络的 $\delta ^l$ 计算前一层网络的 $\delta ^{l-1}$ 

先给出公式：
$$
\delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma^{'}(z^l)\tag{BP2}
$$
$(w^{l+1})^T$是网络第$(l+1)$层权值矩阵的转置。结合$(BP1)$和$(BP2)$，我们可以计算出网络中所有层的$\delta ^l$。

#### 证明

$$
\begin{align}
  \delta^l_j & = \frac{\partial C}{\partial z^l_j} \\
  & = \sum_k \frac{\partial C}{\partial z^{l+1}_k} \frac{\partial z^{l+1}_k}{\partial z^l_j} \text // z_j^l的变化将影响后一层(即l+1层)所有神经元的输出 \\ 
  & = \sum_k \frac{\partial z^{l+1}_k}{\partial z^l_j} \delta^{l+1}_k
\end{align}
$$



### 第三个方程：偏置值的变化对网络损失值的影响

网络中偏置值 $b_j^l$的变化对网络损失值的影响可以使用如下表达式进行计算：
$$
\frac {\partial C}{\partial b_j^l} = \delta_j^l\tag{BP3}
$$
结合$(7)$可证式$(BP3)$：
$$
\frac {\partial C}{\partial b_j^l}= \frac {\partial C}{\partial (\sum_k w_{jk}^l a_{k}^{l-1} + b_j^l)} = \frac {\partial C}{\partial (z_j^l)} = \delta_j^l \tag{9}
$$
从式$(BP3)$可知，我们可以使用$\delta_j^l$来计算偏置值关于损失函数的梯度。

### 第四个方程：权值的变化对网络损失值的影响

$$
\frac {\partial C}{\partial w_{jk}^l} = a_k^{l-1}\delta_j^l\tag{BP4}
$$

$(BP4)$告诉我们，我们可以使用前一层网络的输出和 $\delta_j^l$ 来计算权值关于损失函数的梯度，而这些值都是已知的。

#### 证明

$$
\begin{align}
z_j^l & = \sum_k a_k^{l-1}w_{jk}^l +b_j^l \Rightarrow \frac {\partial z_j^l}{w_{jk}^l} = a_k^{l-1} \\
\frac {\partial C}{\partial w_{jk}^l} &= \frac {\partial C}{\partial z_j^l} \frac {\partial z_j^l}{\partial w_{jk}^l} = \delta_j^l a_k^{l-1}
\end{align}
$$



观察上面几个方程，对于输出层而言，如果$z_j^L$非常大且我们使用的激活函数为$sigmoid(x) = \frac {1}{1-e^{-x}} $，那么$\sigma^{'} \approx 0$，此时$\delta ^L \approx 0$，网络是无法更新权重与偏置的，即网络失去了学习能力。随着网络层数的增加，位于左侧的层其权值与偏置也将非常难以更新，因为$a_k^{l-1}\delta_j^l$值向左传播的过程中会越来越接近于0。因此，好的激活函数对网络的训练是有益的，而且网络的层数也不是越多越好，跨层连接（如ResNet）对网络的训练也是有益的。

### BP所依赖的四个方程总结如下

$$
\begin{align} 
\delta^L &= \nabla_aC \odot \sigma^{'}(z^L) \\
\delta^l &= ((w^{l+1})^T \delta^{l+1}) \odot \sigma^{'}(z^l) \\
\frac {\partial C}{\partial b_j^l} &= \delta_j^l\\
\frac {\partial C}{\partial w_{jk}^l} &= a_k^{l-1}\delta_j^l
\end{align}
$$





[0]:http://neuralnetworksanddeeplearning.com/chap2.html