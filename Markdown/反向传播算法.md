# 反向传播算法简介（BP）

原文地址：[http://neuralnetworksanddeeplearning.com/chap2.html][0]

BP (Backpropagation algorithm) 算法所关注的是神经网络中损失函数 C (cost function) 与每一个权重 $w$ 和偏置 $b$ 的偏导。BP 不仅仅是一个快速的算法，其同时为我们提供了一个视角，让我们观察权值和偏置是如何影响网络输出的。

译者注：本文中所描述的网络以层为单位，如果把层当做图的节点，数据流向作为图的有向边，那么本文所描述的网络所抽象出的图一定是有向无环的。___本文并没有翻译原文所有内容___。

[TOC]

## 1. 热身：利用矩阵实现网络计算

先介绍一种网络权重的数学标记法：$w_{jk}^l$，这个数学标记表示神经网络中第 $l$ 层的第 $j$ 个元素和第 $l-1$ 层第 $k$ 个元素之间的权重。同样，$b_j^l$ 表示网络第 $l$ 层第 $j$ 个元素的偏置值，$a_j^l$ 表示 $l$ 层第 $j$ 个元素的激活函数输出值。利用这种数学标记法，$a_j^l$ 可以表示为：

$$
a_j^l = \sigma(\sum_k w_{jk}^l a_k^{l-1} + b_j^l) \tag{1}
$$

其中 $\sigma(x)$ 为神经元的激活函数，使用矩阵形式表示上述表达式：

$$
a^l = \sigma(w^l a^{l-1} + b^l) \tag{2}
$$

定义 $z^l = w^l a^{l-1} + b^l$ 为神经元激活函数的输入值则可以将上面表达式$(2)$ 表示为：

$$
a^l = \sigma(z^l) \tag{3}
$$

## 2. 损失函数的两个特点

BP 算法用于计算网络中所有权重 $w$ 和偏置 $b$ 关于损失函数 $C$ 的偏导数 $\frac{\partial C}{\partial w}$和$\frac{\partial C}{\partial b}$。为了使 BP 算法正常运行，损失函数需要满足两个条件。在给出这两个条件前，我们先介绍一种常用的均方差损失函数，如式$(4)$所示：

$$
C = \frac 1{2n} \sum_x \Vert y(x) - a^L(x) \Vert ^2 \tag{4}
$$

表达式 $(4)$ 中变量分别为：$n$ 是训练网络的样本个数；$y(x)$ 是训练样本 $x$ 的期望值（也就是样本的标签值）；$L$ 表示网络的层数；$a^L  = a^L(x)$ 是网络在输入为 $x$ 时输出层的输出。

现在描述我们对损失函数的要求。首先，损失函数可以写成所有训练样本损失值均值的形式：$C = \frac {1}{n} \sum_x C_x$ 。

我们做上面的要求是因为训练的过程中我们常常使用批训练的方式，而不是每次只使用一个样本训练网络。批训练之后我们求当前批次样本损失值的平均数来更新权重和偏置，所以损失函数要满足叠加定理。

其次，损失函数可以使用网络输出层的输出作为参数：$C = C(a^L)$，$a^L$ 是网络输出层的输出，如果不满足这个要求我们将不能定量分析网络的性能（因为无法计算网络的损失值）。以均方差损失函数为例，当样本为 $x$ 时，网络的损失值为：

$$
C = \frac 1 2 \Vert y-a^L \Vert ^2 = \frac 1 2 \sum_j(y_j-a_j^L)^2 \tag{5}
$$

上式中所有元素的值都是已知的，$y$ 是标签、$a_j^L$ 是网络输出层的输出。

## 3. Hadamard 积，$s \odot t $

Hadamardd 积（哈达玛积）表示矩阵按对应元素做乘法：$(s \odot t)_j = s_j t_j$，例如：

$$
\begin{vmatrix}
	1 \\
	2 
\end{vmatrix} \odot \begin{vmatrix}
	3 \\
	4 
\end{vmatrix} = \begin{vmatrix}
	1*3 \\
	2*4 
\end{vmatrix} = \begin{vmatrix}
	3 \\
	8 
\end{vmatrix} \tag{6}
$$

## 4. BP 算法所依赖的四个方程

BP 算法用于计算网络中权值与偏置关于网络损失值的偏导，也就是计算：$\frac{\partial C}{\partial w_{jk}^l}$和$\frac{\partial C}{\partial b_j^l}$ 。在计算偏导前我们先引入一个中间变量 $\delta_j^l$，这个变量表示网络第 $l$ 层第 $j$ 个元素的输入值（$z_j^l$）对整个网络损失的影响。BP 算法可以帮我们计算出 $\delta_j^l$ ，然后我们就可以通过 $\delta_j^l$ 得到$\frac{\partial C}{\partial w_{jk}^l}$和$\frac{\partial C}{\partial b_j^l}$ 。

为了便于理解，这里我们假设网络中的某个神经元（第 $l$ 层第 $j$ 个）旁边住着一个小恶魔，它可以帮我们调整神经元的输入（$z_j^l$）。小恶魔不出手时这个神经元的输入为：$ z_j^l$，小恶魔出手后，当前神经元的输入为：$ z_j^l + \Delta z_j^l$，其中$\Delta z_j^l$ 是小恶魔做的调整，此时这个神经元的输出为 $\sigma (z_j^l + \Delta z_j^l)$。小恶魔对当前神经元的调整最终将影响整个网络的输出，小恶魔出手后对整个网络损失值的影响为：$\frac {\partial C} {\partial z_j^l} \Delta z_j^l$。

如果这是一个善良的小恶魔，那么它可以帮我们调整输入以减少网络的损失值。如果$\frac {\partial C} {\partial z_j^l}$ 的值不为0，那么调整$ z_j^l = z_j^l - \eta \frac {\partial C} {\partial z_j^l} \Delta z_j^l$，将减小整个网络的损失值（$\eta$ 是学习率，是个比较小的小数）。如果 $\frac {\partial C} {\partial z_j^l}$ 的值为0，那么小恶魔的调整对网络损失的贡献就非常小，因为导数为 0，再大的调整对网络都没有影响（这里暂不考虑舍入误差）。

我们定义 $\delta_j^l$ 如下：

$$
\delta \equiv \frac {\partial C} {\partial z_j^l} \tag{7}
$$

式$ (7) $的矩阵表示方法为：$\delta ^l$。BP 算法可以帮助我们计算网络中每一层的 $\delta ^l$。

### 4.1 BP1：输出层损失值计算

网络输出层 $\delta ^L$的计算方式为：

$$
\delta_j^L = \frac {\partial C}{\partial a_j^L} \sigma{'}(z_j^l)\tag{BP1}
$$

BP1 等号右侧偏导部分表示网络输出层最后的输出对网络损失的影响强度，$\sigma$ 的导数表示这个神经元输入对整个网络损失的影响强度（下有证明）。

需要注意的是对于最后一层而言，BP1 的结果是很容易计算的。$\frac {\partial C}{\partial a_j^L}$依赖于损失函数的形式。举个例子，如果我们使用式$(5)$中的均方差作为最终的损失函数，那么$\frac {\partial C}{\partial a_j^L} = (a_j^L - y_j)$。

以矩阵的形式表示$(BP1)$：

$$
\delta^L = \nabla _aC \odot \sigma^{'}(z^L) \tag{BP1a}
$$

如果损失函数还是均方差的话，那么 $\nabla _aC = (a^L-y)$，从而可得：

$$
\delta^L = (a^L-y) \odot \sigma^{'}(z^L) \tag{8}
$$

**证明：**  
$$
\begin{align}
\delta^L_j &= \frac{\partial C}{\partial z^L_j} \\
           &= \frac{\partial C}{\partial a^L_j} \frac{\partial a^L_j}{\partial z^L_j}\\
           &=\frac{\partial C}{\partial a^L_j} \sigma'(z^L_j)
\end{align}
$$

### 4.2 BP2：使用 $\delta ^l$ 计算 $\delta ^{l-1}$ 

先给出公式：

$$
\delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma^{'}(z^l)\tag{BP2}
$$

$(w^{l+1})^T$是网络第$(l+1)$层权值矩阵的转置。结合$(BP1)$和$(BP2)$，我们可以计算出网络中所有层的$\delta ^l$。

**证明：**  
$$
\begin{align}
z_j^{l+1} &= \sum_i \sigma(z_i^l)w_{ji}^{l+1}+b_j^l \Rightarrow \frac {\partial z_j^{l+1}}{z_i^l}\Bigg|_{i=k} = \sigma^{'}(z_k^l)w_{jk}^{l+1} \tag{9} \\
\frac {\partial C}{\partial z_k^l} & =\sum_j \frac {\partial C}{\partial z_j^{l+1}} \frac {\partial z_j^{l+1}}{\partial z_k^{l}} // z_j^l 的变化会影响所有 z_k^{l+1}，故 \Delta C 可以使用z_k^{l+1}计算 \tag{10} \\
&=\sum_j \delta_j^{l+1} \frac {\partial z_j^{l+1}}{\partial z_k^{l}}\\ 
&=\sum_j \delta_j^{l+1} \sigma^{'}(z_k^l)w_{jk}^{l+1} 
\end{align}
$$

写成矩阵形式即为式 $(BP2)$。

_译者注_：  

如果将神经网络的每一层都看做一个函数 $y = f(x)$， $x$ 为当前层的输入， $y$ 是当前层的输出，则输入对输出影响的强度即为函数的导数：$\frac {d y}{d x} = f^{'}(x)$。

神经网络一般有很多层，每一层的输入都是前一层的输出（这里只考虑简单的网络，每一层的输入只和相邻的前一层相关），那么一个含有两个隐藏层的网络可以用函数 $y = f(g(x))$ 表示。其中 $x$ 是整个网络的输入，$g$ 表示第一层网络，$f$ 表示第二层网络，$y$ 为整个网络的输出。

在已知第二层网络输入的前提下，$\delta ^{layer 2} = f^{'}(in_{layer 2 input})$。在已知网络第一层输入 $x$ 的前提下，求 $\delta ^{layer 1}$需要使用微积分中的链式求导法则，即：

$$
\delta ^{layer 1} = f^{'}(g(x))g^{'}(x) \tag{11}
$$

式$(11)$中所包含的思想和式$(10)$是相同的，在已知 $x$ 的前提下 $g(x)$ 也是已知的。因为 $f$ 和 $g$ 的函数形式是已知的故其导数形式也是已知的。

综上所述，所有层的 $\delta ^l$都是可以通过链式求导法则进行计算的。

### 4.3 BP3：偏置值对网络损失值的影响

网络中偏置值 $b_j^l$的变化对网络损失值的影响可以使用如下表达式进行计算：

$$
\frac {\partial C}{\partial b_j^l} = \delta_j^l\tag{BP3}
$$

结合$(7)$可证式$(BP3)$：

$$
\frac {\partial C}{\partial b_j^l}= \frac {\partial C}{\partial (\sum_k w_{jk}^l a_{k}^{l-1} + b_j^l)} = \frac {\partial C}{\partial (z_j^l)} = \delta_j^l \tag{12}
$$

从式$(BP3)$可知，我们可以使用$\delta_j^l$来计算偏置值关于损失函数的梯度。

### 4.4 BP4：权值对网络损失值的影响

$$
\frac {\partial C}{\partial w_{jk}^l} = a_k^{l-1}\delta_j^l\tag{BP4}
$$

$(BP4)$告诉我们，我们可以使用前一层网络的输出和 $\delta_j^l$ 来计算权值关于损失函数的梯度，而这些值都是已知的。

**证明：**  

$$
\begin{align}
z_j^l & = \sum_i a_i^{l-1}w_{ji}^l +b_j^l \Rightarrow \frac {\partial z_j^l}{\partial w_{ji}^l} \Bigg|_{i=k} = a_k^{l-1} \\
\frac {\partial C}{\partial w_{jk}^l} &= \frac {\partial C}{\partial z_j^l} \frac {\partial z_j^l}{\partial w_{jk}^l} = \delta_j^l a_k^{l-1}
\end{align}
$$

观察上面几个方程，对于输出层而言，如果$z_j^L$非常大且我们使用的激活函数为$sigmoid(x) = \frac {1}{1-e^{-x}} $，那么$\sigma^{'} \approx 0$，此时$\delta ^L \approx 0$，网络是无法更新权重与偏置的，即网络失去了学习能力。

随着网络层数的增加，位于左侧的层其权值与偏置也将非常难以更新，因为 $a_k^{l-1}\delta_j^l$ 值向左传播的过程中会越来越接近于0。因此，好的激活函数对网络的训练是有益的，而且网络的层数也不是越多越好，跨层连接（如ResNet）对网络的训练也是有益的。

**BP所依赖的四个方程总结如下：**  
$$
\begin{align} 
\delta^L &= \nabla_aC \odot \sigma^{'}(z^L) \tag {BP1} \\
\delta^l &= ((w^{l+1})^T \delta^{l+1}) \odot \sigma^{'}(z^l) \tag{BP2} \\
\frac {\partial C}{\partial b_j^l} &= \delta_j^l \tag{BP3}\\
\frac {\partial C}{\partial w_{jk}^l} &= a_k^{l-1}\delta_j^l \tag{BP4}
\end{align}
$$

## 5 反向传播算法的实现

上小节中的四个方程向我们提供了一个计算网络权值和偏置关于网络损失值梯度的方法，下面使用流程的形式来描述这个方法：

1. 输入 $x$ ：设置输入层的值
2. 前向传导：每一层的输入和输出分别为：$z^l = w^l a^{l-1} + b^l$ 和 $a^l = \sigma(z^l)$
3. $\delta ^L$ ，输出层关于损失值的梯度：$\delta ^L = \nabla_a \odot \sigma^{'}(z^L)$
4. 反向传播误差： $\delta^{l} = ((w^{l+1})^T \delta^{l+1}) \odot   \sigma'(z^{l})$
5. 获得权值和偏置关于损失函数的梯度：$\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j$ 和 $\frac{\partial C}{\partial b^l_j} = \delta^l_j$

## 反向传播的代码示例



















































[0]:http://neuralnetworksanddeeplearning.com/chap2.html